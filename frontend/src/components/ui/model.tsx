/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
*/
import * as THREE from 'three'
import React, { useEffect, useMemo, useRef, useState } from 'react'
import { useGLTF , useFBX, useAnimations, useFaceControls} from '@react-three/drei'
import type { GLTF } from 'three-stdlib'
import { button, useControls } from "leva";
import { useFrame, useLoader } from '@react-three/fiber';


const facialExpressions = {
  default: {},
  smile: {
    browInnerUp: 0.17,
    eyeSquintLeft: 0.4,
    eyeSquintRight: 0.44,
    noseSneerLeft: 0.1700000727403593,
    noseSneerRight: 0.14000002836874015,
    mouthPressLeft: 0.61,
    mouthPressRight: 0.41000000000000003,
  },
  funnyFace: {
    jawLeft: 0.63,
    mouthPucker: 0.53,
    noseSneerLeft: 1,
    noseSneerRight: 0.39,
    mouthLeft: 1,
    eyeLookUpLeft: 1,
    eyeLookUpRight: 1,
    cheekPuff: 0.9999924982764238,
    mouthDimpleLeft: 0.414743888682652,
    mouthRollLower: 0.32,
    mouthSmileLeft: 0.35499733688813034,
    mouthSmileRight: 0.35499733688813034,
  },
  sad: {
    mouthFrownLeft: 1,
    mouthFrownRight: 1,
    mouthShrugLower: 0.78341,
    browInnerUp: 0.452,
    eyeSquintLeft: 0.72,
    eyeSquintRight: 0.75,
    eyeLookDownLeft: 0.5,
    eyeLookDownRight: 0.5,
    jawForward: 1,
  },
  surprised: {
    eyeWideLeft: 0.5,
    eyeWideRight: 0.5,
    jawOpen: 0.351,
    mouthFunnel: 1,
    browInnerUp: 1,
  },
  angry: {
    browDownLeft: 1,
    browDownRight: 1,
    eyeSquintLeft: 1,
    eyeSquintRight: 1,
    jawForward: 1,
    jawLeft: 1,
    mouthShrugLower: 1,
    noseSneerLeft: 1,
    noseSneerRight: 0.42,
    eyeLookDownLeft: 0.16,
    eyeLookDownRight: 0.16,
    cheekSquintLeft: 1,
    cheekSquintRight: 1,
    mouthClose: 0.23,
    mouthFunnel: 0.63,
    mouthDimpleRight: 1,
  },
  crazy: {
    browInnerUp: 0.9,
    jawForward: 1,
    noseSneerLeft: 0.5700000000000001,
    noseSneerRight: 0.51,
    eyeLookDownLeft: 0.39435766259644545,
    eyeLookUpRight: 0.4039761421719682,
    eyeLookInLeft: 0.9618479575523053,
    eyeLookInRight: 0.9618479575523053,
    jawOpen: 0.9618479575523053,
    mouthDimpleLeft: 0.9618479575523053,
    mouthDimpleRight: 0.9618479575523053,
    mouthStretchLeft: 0.27893590769016857,
    mouthStretchRight: 0.2885543872656917,
    mouthSmileLeft: 0.5578718153803371,
    mouthSmileRight: 0.38473918302092225,
    tongueOut: 0.9618479575523053,
  },
};
const corresponding = {
  A: "viseme_PP",
  B: "viseme_kk",
  C: "viseme_I",
  D: "viseme_AA",
  E: "viseme_O",
  F: "viseme_U",
  G: "viseme_FF",
  H: "viseme_TH",
  X: "viseme_PP",
};

type GLTFResult = GLTF & {
  nodes: {
    EyeLeft: THREE.SkinnedMesh
    EyeRight: THREE.SkinnedMesh
    Wolf3D_Head: THREE.SkinnedMesh
    Wolf3D_Teeth: THREE.SkinnedMesh
    Wolf3D_Body: THREE.SkinnedMesh
    Wolf3D_Outfit_Bottom: THREE.SkinnedMesh
    Wolf3D_Outfit_Footwear: THREE.SkinnedMesh
    Wolf3D_Outfit_Top: THREE.SkinnedMesh
    Hips: THREE.Bone
  }
  materials: {
    Wolf3D_Eye: THREE.MeshStandardMaterial
    Wolf3D_Skin: THREE.MeshStandardMaterial
    Wolf3D_Teeth: THREE.MeshStandardMaterial
    Wolf3D_Body: THREE.MeshStandardMaterial
    Wolf3D_Outfit_Bottom: THREE.MeshStandardMaterial
    Wolf3D_Outfit_Footwear: THREE.MeshStandardMaterial
    Wolf3D_Outfit_Top: THREE.MeshStandardMaterial
  }
}

export default function Model({config,
  onFirstResponse,  
  ...props
}: {
  config?: any
  onFirstResponse?: () => void  
} & JSX.IntrinsicElements['group']) {


const { nodes, materials ,scene } = useGLTF('/models/68852c684c405a37ad57df05.glb') as GLTFResult
const {animations : talkingAnimation } = useFBX('/models/Talking.fbx') 
talkingAnimation[0].name = 'Talking'
const [animation , setAnimation] = useState('Talking') ; 
const group = useRef<THREE.Group>(null);
const { actions , mixer } = useAnimations(
  [talkingAnimation[0]], group
);
const [isLoading, setIsLoading] = useState(true)
useEffect(() => {
    console.log(animation)
    console.log(actions[animation]) 
    actions[animation]?.reset().fadeIn(0.5).play() ;
    console.log('Playing animation:', animation) ;
    return () => {
      actions[animation]?.fadeOut(0.5) ;
    console.log('Stopping animation:', animation) ;
    }
  }, [animation]) ; 

 const {playAudio,script} = useControls( {
  playAudio: false,
  script: {
    value: "welcome",
    options: ['welcome']
  }}) 

 useControls("FacialExpressions", { 
    winkLeft: button(() => {
      setWinkLeft(true);
      setTimeout(() => setWinkLeft(false), 300);
    }),
    winkRight: button(() => {
      setWinkRight(true);
      setTimeout(() => setWinkRight(false), 300);
    }),
    animation: {
      value: animation,
      options: animation,
    },
    facialExpression: {
      options: Object.keys(facialExpressions),
      onChange: (value) => setFacialExpression(value),
    },
    enableSetupMode: button(() => {
      setupMode = true;
    }),
    disableSetupMode: button(() => {
      setupMode = false;
    }),
    logMorphTargetValues: button(() => {
      const emotionValues = {};
      Object.keys(nodes.EyeLeft.morphTargetDictionary).forEach((key) => {
        if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
          return; // eyes wink/blink are handled separately
        }
        const value =
          nodes.EyeLeft.morphTargetInfluences[
            nodes.EyeLeft.morphTargetDictionary[key]
          ];
        if (value > 0.01) {
          emotionValues[key] = value;
        }
      });
      console.log(JSON.stringify(emotionValues, null, 2));
    }),
    startListening: button(() => startMicrophone()),
    stopListening: button(() => stopMicrophone()),
  });
const [, set] = useControls("MorphTarget", () =>
Object.assign(
  {},
  ...Object.keys(nodes.EyeLeft.morphTargetDictionary).map((key) => {
    return {
      [key]: {
        label: key,
        value: 0,
        min: nodes.EyeLeft.morphTargetInfluences[
          nodes.EyeLeft.morphTargetDictionary[key]
        ],
        max: 1,
        onChange: (val) => {
          if (setupMode) {
            lerpMorphTarget(key, val, 1);
          }
        },
      },
    };
  })
)
);

  
const [blink, setBlink] = useState(false);
const [winkLeft, setWinkLeft] = useState(false);
const [winkRight, setWinkRight] = useState(false);
const [facialExpression, setFacialExpression] = useState("");
const [audio, setAudio] = useState();
const [lipsync, setLipsync] = useState();
const [message,setMessage] = useState({
    "text": "",
    "audio": "",
    "lipsync": {
        "metadata": {
            "soundFile": "",
            "duration": 0
        },
        "mouthCues": [
            
        ]
    },
    "facialExpression": "smile",
    "animation": "smile"
})
const [isListening, setIsListening] = useState(false)
const wsRef = useRef<WebSocket | null>(null)
const audioContextRef = useRef<AudioContext | null>(null)
const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null)
const processorRef = useRef<ScriptProcessorNode | null>(null)
const streamRef = useRef<MediaStream | null>(null)
/* 
const AI_SIMULATION_MODEL = process.env.NEXT_PUBLIC_AI_SIMULATION_MODEL ;
console.log("AI_SIMULATION_MODEL:", AI_SIMULATION_MODEL);
const fetchMessage = async () => {
  try {
    const response = await fetch(AI_SIMULATION_MODEL, {
      method: "POST", 
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        message_id: "api_2",
        text: "hello this is a test what language do you speak",
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data = await response.json();
    setMessage(data);  // data should contain: text, audio, animation, facialExpression, lipsync
    console.log("Message fetched:", data);
  } catch (error) {
    console.error("Failed to fetch message:", error);
  }
}; 
 */
/* const audio = useMemo(() => new Audio('/Audios/welcome.mp3'), [script]); 
const jsonFile = useLoader(THREE.FileLoader, '/Audios/welcome.json') ;
const lipsync = JSON.parse(jsonFile) ;  */

useFrame(() => { 
  /* const currentAudioTime = audio.currentTime;
  Object.values(corresponding).forEach((value) => {
    nodes.Wolf3D_Head.morphTargetInfluences[
    nodes.Wolf3D_Head.morphTargetDictionary[value]
  ] = 0 ;
  nodes.Wolf3D_Teeth.morphTargetInfluences[
    nodes.Wolf3D_Teeth.morphTargetDictionary[value]
  ] = 0 ;

  })

  for (let i = 0; i < lipsync.mouthCues.length; i++) {
    const mouthCue = lipsync.mouthCues[i];
    if (currentAudioTime>=mouthCue.start &&currentAudioTime<=mouthCue.end){
      console.log('Mouth Cue:', mouthCue.value, 'at time:', currentAudioTime);
      nodes.Wolf3D_Head.morphTargetInfluences[
      nodes.Wolf3D_Head.morphTargetDictionary[corresponding[mouthCue.value]]
      ] = 1 ;
      nodes.Wolf3D_Teeth.morphTargetInfluences[
      nodes.Wolf3D_Teeth.morphTargetDictionary[corresponding[mouthCue.value]]
      ] = 1 ;
      break; // Exit loop after finding the first matching cue
    }
  } */
  !setupMode &&
      Object.keys(nodes.EyeLeft.morphTargetDictionary).forEach((key) => {
        const mapping = facialExpressions[facialExpression];
        if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
          return;
        }
        if (mapping && mapping[key]) {
          lerpMorphTarget(key, mapping[key], 0.1);
        } else {
          lerpMorphTarget(key, 0, 0.1);
        }
      });
    lerpMorphTarget("eyeBlinkLeft", blink || winkLeft ? 1 : 0, 0.5);
    lerpMorphTarget("eyeBlinkRight", blink || winkRight ? 1 : 0, 0.5);
    if (setupMode) {
      return;
    }

const appliedMorphTargets: string[] = [];
    if ( lipsync) {
      const currentAudioTime = audio.currentTime;
      for (let i = 0; i < lipsync.mouthCues.length; i++) {
        const mouthCue = lipsync.mouthCues[i];
        if (
          currentAudioTime >= mouthCue.start &&
          currentAudioTime <= mouthCue.end
        ) {
          appliedMorphTargets.push(corresponding[mouthCue.value]);
          lerpMorphTarget(corresponding[mouthCue.value], 1, 0.2);
          break;
        }
      }
    }

    Object.values(corresponding).forEach((value) => {
      if (appliedMorphTargets.includes(value)) {
        return;
      }
      lerpMorphTarget(value, 0, 0.1);
    });

 })

let setupMode = false;
const lerpMorphTarget = (
  target: string,
  value: number,
  speed: number = 0.1
) => {
  scene.traverse((child) => {
    // Check and type guard for SkinnedMesh
    if (
      (child as THREE.SkinnedMesh).isSkinnedMesh &&
      (child as THREE.SkinnedMesh).morphTargetDictionary
    ) {
      const skinned = child as THREE.SkinnedMesh;

      const index = skinned.morphTargetDictionary![target];
      if (
        index === undefined ||
        skinned.morphTargetInfluences?.[index] === undefined
      ) {
        return;
      }

      skinned.morphTargetInfluences[index] = THREE.MathUtils.lerp(
        skinned.morphTargetInfluences[index],
        value,
        speed
      );

      /* if (!setupMode) {
        try {
          set({
            [target]: value,
          });
        } catch (e) {
          // silence
        }
      } */
    }
  });
};


/*  useEffect(() => {
  console.log(nodes.Wolf3D_Teeth.morphTargetDictionary) ; 
  nodes.Wolf3D_Head.morphTargetInfluences[
    nodes.Wolf3D_Head.morphTargetDictionary["viseme_aa"]
  ] = 0 ;
  nodes.Wolf3D_Teeth.morphTargetInfluences[
    nodes.Wolf3D_Teeth.morphTargetDictionary["viseme_aa"]
  ] = 0 }, []);  */

/* useEffect(() => {
  console.log(nodes.Wolf3D_Teeth.morphTargetDictionary) ;
  console.log(nodes.EyeLeft.morphTargetDictionary) ;
  console.log(nodes.EyeRight.morphTargetDictionary) ;
  console.log(nodes.Wolf3D_Head.morphTargetDictionary) ;
  if (playAudio) {
    audio.play().catch((error) => console.error('Error playing audio:', error));
    } else {
      audio.pause();
      audio.currentTime = 0; // Reset audio to the beginning
    }
  }, [playAudio, script]);
 */
  

useEffect(() => {
  console.log('Blinking started'); ;
    let blinkTimeout: ReturnType<typeof setTimeout>;
    const nextBlink = () => {
      blinkTimeout = setTimeout(() => {
        setBlink(true);
        setTimeout(() => {
          setBlink(false);
          nextBlink();
        }, 200);
      }, THREE.MathUtils.randInt(1000, 5000));
    };
    nextBlink();
    return () => clearTimeout(blinkTimeout);
  }, []);

useEffect(()=>{
  //fetchMessage();
  console.log(message); 
},[])
useEffect(() => {
    console.log(message);
    if (!message) {
      setAnimation("Idle");
      return;
    }
    //setAnimation(message.animation);
    setFacialExpression(message.facialExpression);
    setLipsync(message.lipsync);
    const audio = new Audio("data:audio/mp3;base64," + message.audio);
    audio.play();
    setAudio(audio);
    //audio.onended = onMessagePlayed;
  }, [message]);






/*Microphone handling*/

  // WebSocket and Microphone Setup
useEffect(() => {
  const ws = new WebSocket('ws://localhost:8000/transcribe')
  wsRef.current = ws
  startMicrophone() ;
  ws.onopen = () => {
    console.log('WebSocket connected')
    ws.send(JSON.stringify(config));
  }

  ws.onmessage = (event) => {
    console.log('Receivexd transcription:', event.data);
    try {
      const data = JSON.parse(event.data);
      if (data.status === "config_received") {
        console.log("Config accepted â†’ Starting mic & hiding loading")
        startMicrophone()           
        setIsLoading(false)         
        onFirstResponse?.()         
        return
      }
      setMessage(data?.response);
      console.log(data);
    } catch (error) {
      console.error('Failed to parse WebSocket message:', error);
    }
  }

  ws.onclose = () => {
    console.log('WebSocket disconnected')
    stopMicrophone()
    /*setTimeout(() => {
      wsRef.current = new WebSocket('ws://localhost:9000/transcribe')
    }, 5000) */
  }

  ws.onerror = (error) => {
    console.error('WebSocket error:', error)
  }

  return () => {
    ws.close()
    stopMicrophone()
  }
}, [])
const startMicrophone = async () => {
  try {
    streamRef.current = await navigator.mediaDevices.getUserMedia({ audio: { sampleRate: 16000 } })
    audioContextRef.current = new AudioContext({ sampleRate: 16000 })
    sourceRef.current = audioContextRef.current.createMediaStreamSource(streamRef.current)
    processorRef.current = audioContextRef.current.createScriptProcessor(1024, 1, 1)

    processorRef.current.onaudioprocess = (e) => {
      if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
        const audioData = e.inputBuffer.getChannelData(0)
        wsRef.current.send(audioData.buffer)
      }
    }

    sourceRef.current.connect(processorRef.current)
    processorRef.current.connect(audioContextRef.current.destination)
    setIsListening(true)
  } catch (err) {
    console.error('Error accessing microphone:', err)
  }
}

const stopMicrophone = () => {
  if (processorRef.current) processorRef.current.disconnect()
  if (sourceRef.current) sourceRef.current.disconnect()
  if (audioContextRef.current) audioContextRef.current.close()
  if (streamRef.current) {
    streamRef.current.getTracks().forEach(track => track.stop())
  }
  streamRef.current = null
  audioContextRef.current = null
  sourceRef.current = null
  processorRef.current = null
  setIsListening(false)
}

return (
<group {...props} dispose={null} ref={group}> 
  <primitive object={nodes.Hips} />
  <skinnedMesh
    name="EyeLeft"
    geometry={nodes.EyeLeft.geometry}
    material={materials.Wolf3D_Eye}
    skeleton={nodes.EyeLeft.skeleton}
    morphTargetDictionary={nodes.EyeLeft.morphTargetDictionary}
    morphTargetInfluences={nodes.EyeLeft.morphTargetInfluences}
  />
  <skinnedMesh
    name="EyeRight"
    geometry={nodes.EyeRight.geometry}
    material={materials.Wolf3D_Eye}
    skeleton={nodes.EyeRight.skeleton}
    morphTargetDictionary={nodes.EyeRight.morphTargetDictionary}
    morphTargetInfluences={nodes.EyeRight.morphTargetInfluences}
  />
  <skinnedMesh
    name="Wolf3D_Head"
    geometry={nodes.Wolf3D_Head.geometry}
    material={materials.Wolf3D_Skin}
    skeleton={nodes.Wolf3D_Head.skeleton}
    morphTargetDictionary={nodes.Wolf3D_Head.morphTargetDictionary}
    morphTargetInfluences={nodes.Wolf3D_Head.morphTargetInfluences}
  />
  <skinnedMesh
    name="Wolf3D_Teeth"
    geometry={nodes.Wolf3D_Teeth.geometry}
    material={materials.Wolf3D_Teeth}
    skeleton={nodes.Wolf3D_Teeth.skeleton}
    morphTargetDictionary={nodes.Wolf3D_Teeth.morphTargetDictionary}
    morphTargetInfluences={nodes.Wolf3D_Teeth.morphTargetInfluences}
  />
  <skinnedMesh
    geometry={nodes.Wolf3D_Body.geometry}
    material={materials.Wolf3D_Body}
    skeleton={nodes.Wolf3D_Body.skeleton}
  />
  <skinnedMesh
    geometry={nodes.Wolf3D_Outfit_Bottom.geometry}
    material={materials.Wolf3D_Outfit_Bottom}
    skeleton={nodes.Wolf3D_Outfit_Bottom.skeleton}
  />
  <skinnedMesh
    geometry={nodes.Wolf3D_Outfit_Footwear.geometry}
    material={materials.Wolf3D_Outfit_Footwear}
    skeleton={nodes.Wolf3D_Outfit_Footwear.skeleton}
  />
  <skinnedMesh
    geometry={nodes.Wolf3D_Outfit_Top.geometry}
    material={materials.Wolf3D_Outfit_Top}
    skeleton={nodes.Wolf3D_Outfit_Top.skeleton}
  />
</group>
)
}

useGLTF.preload('/models/68852c684c405a37ad57df05.glb')
useGLTF.preload("/models/Talking.fbx");
